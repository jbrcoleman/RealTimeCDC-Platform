# RealTimeCDC-Platform

A production-ready, real-time Change Data Capture (CDC) platform built on AWS EKS, demonstrating modern data streaming architecture with Kafka, Debezium, Flink, and GitOps practices.

## ğŸ¯ Project Overview

This platform captures database changes in real-time from PostgreSQL and streams them to multiple destinations for different use cases:
- **Data Lake (S3)**: Historical analytics and compliance
- **DynamoDB**: Fast materialized views for application reads
- **Stream Processing (Flink)**: Real-time transformations and aggregations
- **Consumer Microservices**: Event-driven Python applications

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚  (Source: E-commerce Database)
â”‚   RDS       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Logical Replication
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         EKS Cluster (Kubernetes)           â”‚
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Debezium â”‚â”€â”€â”€â†’â”‚   Kafka   â”‚             â”‚
â”‚  â”‚  (CDC)   â”‚    â”‚ (Strimzi) â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                        â”‚                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚         â†“              â†“              â†“    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Flink  â”‚  â”‚ Consumer â”‚  â”‚ Consumer â”‚  â”‚
â”‚  â”‚  Jobs    â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“             â†“             â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   S3   â”‚    â”‚ DynamoDBâ”‚   â”‚    S3    â”‚
   â”‚  Lake  â”‚    â”‚  Tables â”‚   â”‚   DLQ    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Technologies Used

### Infrastructure
- **AWS EKS (Kubernetes 1.33)**: Container orchestration with Karpenter autoscaling
- **Terraform**: Infrastructure as Code for AWS resources
- **ArgoCD**: GitOps continuous delivery for Kubernetes
- **Karpenter**: Intelligent node autoscaling with spot instances

### Data Streaming
- **Apache Kafka 4.1.0 (Strimzi 0.48.0)**: Distributed event streaming (KRaft mode)
- **Debezium**: Change data capture connector for PostgreSQL
- **Apache Flink**: Stream processing framework
- **Schema Registry**: Avro schema management

### Storage & Databases
- **Amazon RDS (PostgreSQL 16)**: Source transactional database
- **Amazon S3**: Data lake and dead letter queue storage
- **Amazon DynamoDB**: Materialized views for fast lookups

### Application Runtime
- **Python 3.11+**: Consumer microservices
- **FastAPI**: RESTful APIs for consumers
- **Boto3**: AWS SDK for Python

## ğŸ“ Repository Structure

```
RealTimeCDC-Platform/
â”œâ”€â”€ terraform-infra/              # Infrastructure Layer (Terraform)
â”‚   â”œâ”€â”€ eks.tf                    # EKS cluster configuration
â”‚   â”œâ”€â”€ rds.tf                    # PostgreSQL with CDC enabled
â”‚   â”œâ”€â”€ s3.tf                     # S3 buckets (data lake, DLQ)
â”‚   â”œâ”€â”€ iam.tf                    # IAM roles and policies
â”‚   â””â”€â”€ vpc.tf                    # Network configuration
â”‚
â”œâ”€â”€ terraform-apps/               # Application Layer (Terraform)
â”‚   â”œâ”€â”€ argocd.tf                 # ArgoCD installation
â”‚   â”œâ”€â”€ alb-controller.tf         # AWS Load Balancer Controller
â”‚   â”œâ”€â”€ karpenter-nodepools.tf    # Karpenter node pools
â”‚   â”œâ”€â”€ pod-identities.tf         # Pod Identity associations
â”‚   â”œâ”€â”€ route53.tf                # DNS records for ingress
â”‚   â””â”€â”€ strimzi.tf                # Strimzi operator (managed via script)
â”‚
â”œâ”€â”€ argocd/                       # GitOps Configuration
â”‚   â”œâ”€â”€ bootstrap/
â”‚   â”‚   â””â”€â”€ root-app.yaml         # App of Apps pattern
â”‚   â”œâ”€â”€ applications/             # Application definitions
â”‚   â”‚   â”œâ”€â”€ consumers.yaml        # Consumer microservices
â”‚   â”‚   â”œâ”€â”€ flink-jobs.yaml       # Flink stream processing
â”‚   â”‚   â”œâ”€â”€ ingress.yaml          # Ingress resources
â”‚   â”‚   â””â”€â”€ kafka-cluster.yaml    # Kafka cluster
â”‚   â””â”€â”€ app-manifests/            # Kubernetes manifests
â”‚       â”œâ”€â”€ consumers/            # Consumer deployments
â”‚       â”œâ”€â”€ flink/                # Flink job/task managers
â”‚       â”œâ”€â”€ ingress/              # ALB ingress configs
â”‚       â””â”€â”€ kafka/                # Kafka cluster configs
â”‚
â”œâ”€â”€ apps/                         # Application Code
â”‚   â”œâ”€â”€ consumers/                # Python consumer services
â”‚   â”‚   â”œâ”€â”€ analytics-service/
â”‚   â”‚   â”œâ”€â”€ inventory-service/
â”‚   â”‚   â””â”€â”€ search-indexer/
â”‚   â””â”€â”€ flink/                    # Flink jobs (Java/Scala)
â”‚       â”œâ”€â”€ sales-aggregations/
â”‚       â”œâ”€â”€ anomaly-detection/
â”‚       â”œâ”€â”€ customer-segmentation/
â”‚       â””â”€â”€ inventory-optimizer/
â”‚
â”œâ”€â”€ scripts/                      # Operational Scripts
â”‚   â”œâ”€â”€ install-kafka.sh          # Install Kafka operator + cluster
â”‚   â”œâ”€â”€ install-debezium.sh       # Deploy Debezium connectors
â”‚   â”œâ”€â”€ init-database.sh          # Initialize source database
â”‚   â”œâ”€â”€ build-flink-jobs.sh       # Build and push Flink jobs
â”‚   â”œâ”€â”€ submit-flink-job.sh       # Submit Flink job to cluster
â”‚   â”œâ”€â”€ cleanup-all.sh            # Comprehensive cleanup
â”‚   â”œâ”€â”€ cleanup-dynamodb.sh       # Clean DynamoDB tables
â”‚   â””â”€â”€ teardown.sh               # Full teardown
â”‚
â””â”€â”€ docs/                         # Additional Documentation
    â””â”€â”€ *.md                      # Detailed guides
```

## ğŸ—ï¸ Deployment Architecture

This platform uses a **hybrid approach** combining the best of Terraform, GitOps, and scripts:

### Infrastructure Layer (Terraform)
- **terraform-infra/**: Core AWS infrastructure (EKS, RDS, S3, VPC, IAM)
- Stable, rarely changes
- Deployed once during initial setup

### Application Layer (Terraform + GitOps)
- **terraform-apps/**: Kubernetes operators and controllers (ArgoCD, ALB Controller, Karpenter)
- **argocd/**: Application deployments via GitOps (consumers, Flink jobs, ingress)
- Auto-synced, drift detection enabled

### Kafka Infrastructure (Script-based)
- **scripts/install-kafka.sh**: Deploys Strimzi operator and Kafka cluster
- Helm-based for flexibility and compatibility
- Kafka resources (topics, users) managed by ArgoCD

## ğŸš€ Quick Start

### Prerequisites

1. **AWS Account** with appropriate permissions
2. **AWS CLI** configured with credentials
3. **Terraform** >= 1.5.0
4. **kubectl** >= 1.27
5. **Helm** >= 3.12
6. **Git** for version control

### Step 1: Deploy Infrastructure

```bash
# Clone the repository
git clone https://github.com/your-org/RealTimeCDC-Platform.git
cd RealTimeCDC-Platform

# Deploy core infrastructure (EKS, RDS, S3, VPC)
cd terraform-infra
terraform init
terraform plan
terraform apply

# Save outputs for later use
terraform output > ../infrastructure-outputs.txt
cd ..
```

### Step 2: Deploy Application Layer

```bash
# Deploy Kubernetes applications layer (ArgoCD, ALB Controller, Karpenter)
cd terraform-apps

# Update terraform.tfvars with your values
cat > terraform.tfvars <<EOF
environment     = "dev"
git_repo_url    = "https://github.com/YOUR-ORG/RealTimeCDC-Platform"
git_revision    = "main"
domain_name     = "your-domain.com"
certificate_arn = "arn:aws:acm:region:account:certificate/xxx"
EOF

terraform init
terraform plan
terraform apply

cd ..
```

### Step 3: Install Kafka Infrastructure

```bash
# Install Strimzi operator and Kafka cluster
./scripts/install-kafka.sh

# Verify Kafka cluster is ready
kubectl get kafka -n kafka
kubectl get pods -n kafka
```

### Step 4: Initialize Database

```bash
# Create sample e-commerce database and enable CDC
./scripts/init-database.sh
```

### Step 5: Deploy Debezium Connectors

```bash
# Deploy Debezium CDC connectors
./scripts/install-debezium.sh

# Verify connectors are running
kubectl get kafkaconnector -n kafka
```

### Step 6: Build and Deploy Flink Jobs

```bash
# Build Flink job Docker images
./scripts/build-flink-jobs.sh

# Submit Flink jobs to the cluster
./scripts/submit-flink-job.sh sales-aggregations
./scripts/submit-flink-job.sh anomaly-detection
./scripts/submit-flink-job.sh customer-segmentation
./scripts/submit-flink-job.sh inventory-optimizer
```

### Step 7: Verify Deployment

```bash
# Check all pods are running
kubectl get pods -A

# Access ArgoCD UI
echo "ArgoCD URL: https://argocd.your-domain.com"
kubectl -n argocd get secret argocd-initial-admin-secret \
  -o jsonpath='{.data.password}' | base64 -d

# Access Flink Dashboard
echo "Flink URL: https://flink.your-domain.com"

# Check Kafka topics
kubectl exec -it cdc-platform-kafka-brokers-0 -n kafka -- \
  bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
```

## ğŸ”„ GitOps Workflow

The platform uses ArgoCD for automated deployments:

1. **Make changes** to application manifests in `argocd/app-manifests/`
2. **Commit and push** to Git
3. **ArgoCD automatically syncs** changes to the cluster
4. **Monitor** via ArgoCD UI at `https://argocd.your-domain.com`

### Manual Sync (if needed)

```bash
# Sync specific application
kubectl patch application consumer-apps -n argocd \
  --type merge -p '{"operation":{"sync":{"revision":"main"}}}'

# Sync all applications
kubectl patch application root-app -n argocd \
  --type merge -p '{"operation":{"sync":{"revision":"main"}}}'
```

## ğŸ§¹ Cleanup

### Option 1: Clean up applications (keep infrastructure)

```bash
# Comprehensive cleanup of apps and Kafka
./scripts/cleanup-all.sh
```

### Option 2: Full teardown (including infrastructure)

```bash
# Clean up everything including EKS cluster
./scripts/cleanup-all.sh

# Destroy application layer
cd terraform-apps
terraform destroy

# Destroy infrastructure layer
cd ../terraform-infra
terraform destroy
```

## ğŸ“Š Monitoring & Observability

### Access Dashboards

```bash
# ArgoCD - GitOps Dashboard
https://argocd.your-domain.com

# Flink - Stream Processing Dashboard
https://flink.your-domain.com

# Kafka - Topic and Consumer Metrics
kubectl port-forward -n kafka svc/cdc-platform-kafka-exporter 9308:9308
```

### Check Kafka Consumer Lag

```bash
kubectl exec -it cdc-platform-kafka-brokers-0 -n kafka -- \
  bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --describe --all-groups
```

### View Debezium Connector Status

```bash
kubectl get kafkaconnector -n kafka -o wide
```

## ğŸ› ï¸ Troubleshooting

### Kafka Issues

```bash
# Check Strimzi operator logs
kubectl logs -n kafka deployment/strimzi-cluster-operator

# Check Kafka broker logs
kubectl logs -n kafka cdc-platform-kafka-brokers-0

# Verify Kafka cluster status
kubectl get kafka cdc-platform -n kafka -o yaml
```

### ArgoCD Sync Issues

```bash
# Check application status
kubectl get applications -n argocd

# View sync errors
kubectl describe application consumer-apps -n argocd

# Force refresh
kubectl patch application consumer-apps -n argocd \
  --type merge -p '{"operation":{"initiatedBy":{"username":"admin"}}}'
```

### Pod Identity Issues

```bash
# Verify pod identity associations
aws eks list-pod-identity-associations --cluster-name cdc-platform

# Check service account annotations
kubectl get sa -n kafka kafka-connect -o yaml
```

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ”— Additional Resources

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Strimzi Operator Guide](https://strimzi.io/docs/operators/latest/deploying.html)
- [Debezium PostgreSQL Connector](https://debezium.io/documentation/reference/connectors/postgresql.html)
- [Apache Flink Documentation](https://flink.apache.org/docs/stable/)
- [ArgoCD Getting Started](https://argo-cd.readthedocs.io/en/stable/getting_started/)
- [Karpenter Best Practices](https://karpenter.sh/docs/getting-started/)

---
