# RealTimeCDC-Platform
Real time change data capture platform
# RealTimeCDC-Platform

A production-ready, real-time Change Data Capture (CDC) platform built on AWS EKS, demonstrating modern data streaming architecture with Kafka, Debezium, and GitOps practices.

## üéØ Project Overview

This platform captures database changes in real-time from PostgreSQL and streams them to multiple destinations for different use cases:
- **Data Lake (S3)**: Historical analytics and compliance
- **DynamoDB**: Fast materialized views for application reads
- **Stream Processing (Flink)**: Real-time transformations and aggregations
- **Consumer Microservices**: Event-driven Python applications

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgreSQL  ‚îÇ  (Source: E-commerce Database)
‚îÇ   RDS       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Logical Replication
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         EKS Cluster (Kubernetes)             ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ Debezium ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ   Kafka   ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ  (CDC)   ‚îÇ    ‚îÇ (Strimzi) ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                        ‚îÇ                     ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ         ‚Üì              ‚Üì              ‚Üì     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Flink  ‚îÇ  ‚îÇ Consumer ‚îÇ  ‚îÇ Consumer ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Jobs    ‚îÇ  ‚îÇ Service  ‚îÇ  ‚îÇ Service  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì             ‚Üì             ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   S3   ‚îÇ    ‚îÇ DynamoDB‚îÇ   ‚îÇ    S3    ‚îÇ
   ‚îÇ  Lake  ‚îÇ    ‚îÇ  Tables ‚îÇ   ‚îÇ   DLQ    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Technologies Used

### Infrastructure
- **AWS EKS**: Kubernetes orchestration with Karpenter autoscaling
- **Terraform**: Infrastructure as Code for AWS resources
- **ArgoCD**: GitOps continuous delivery for Kubernetes

### Data Streaming
- **Apache Kafka (Strimzi)**: Distributed event streaming platform
- **Debezium**: Change data capture connector for PostgreSQL
- **Apache Flink**: Stream processing framework
- **Schema Registry**: Avro schema management

### Storage & Databases
- **Amazon RDS (PostgreSQL 16)**: Source transactional database
- **Amazon S3**: Data lake and dead letter queue storage
- **Amazon DynamoDB**: Materialized views for fast lookups

### Application Runtime
- **Python 3.11+**: Consumer microservices
- **FastAPI**: RESTful APIs for consumers
- **Boto3**: AWS SDK for Python

### Observability
- **Prometheus**: Metrics collection
- **Grafana**: Dashboards and visualization
- **CloudWatch**: AWS native monitoring

## üìÅ Repository Structure

```
RealTimeCDC-Platform/
‚îú‚îÄ‚îÄ terraform/                      # Infrastructure as Code
‚îÇ   ‚îú‚îÄ‚îÄ eks.tf                     # EKS cluster with Karpenter
‚îÇ   ‚îú‚îÄ‚îÄ rds.tf                     # PostgreSQL with CDC enabled
‚îÇ   ‚îú‚îÄ‚îÄ s3.tf                      # Data lake and storage buckets
‚îÇ   ‚îú‚îÄ‚îÄ iam.tf                     # IRSA roles for Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ vpc.tf                     # Networking configuration
‚îÇ   ‚îú‚îÄ‚îÄ karpenter.tf               # Node autoscaling
‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf                 # Terraform outputs
‚îÇ
‚îú‚îÄ‚îÄ k8s/                           # Kubernetes manifests
‚îÇ   ‚îú‚îÄ‚îÄ namespaces/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ namespaces.yaml       # All namespace definitions
‚îÇ   ‚îú‚îÄ‚îÄ service-accounts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service-accounts.yaml # IRSA-enabled service accounts
‚îÇ   ‚îú‚îÄ‚îÄ karpenter/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ karpenter.yaml        # Node pools and classes
‚îÇ   ‚îú‚îÄ‚îÄ kafka/                     # Kafka cluster configs (to be added)
‚îÇ   ‚îú‚îÄ‚îÄ debezium/                  # CDC connector configs (to be added)
‚îÇ   ‚îú‚îÄ‚îÄ flink/                     # Stream processing jobs (to be added)
‚îÇ   ‚îî‚îÄ‚îÄ consumers/                 # Consumer microservices (to be added)
‚îÇ
‚îú‚îÄ‚îÄ argocd/                        # GitOps configuration
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ root-app.yaml         # App of Apps pattern
‚îÇ   ‚îú‚îÄ‚îÄ projects/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cdc-platform.yaml     # ArgoCD project with RBAC
‚îÇ   ‚îî‚îÄ‚îÄ apps/
‚îÇ       ‚îú‚îÄ‚îÄ kafka.yaml            # Kafka application
‚îÇ       ‚îú‚îÄ‚îÄ debezium.yaml         # Debezium application
‚îÇ       ‚îú‚îÄ‚îÄ consumers.yaml        # Consumer applications
‚îÇ       ‚îú‚îÄ‚îÄ flink.yaml            # Flink applications
‚îÇ       ‚îî‚îÄ‚îÄ monitoring.yaml       # Observability stack
‚îÇ
‚îú‚îÄ‚îÄ apps/                          # Application source code (to be added)
‚îÇ   ‚îî‚îÄ‚îÄ python/
‚îÇ       ‚îú‚îÄ‚îÄ inventory-service/    # Real-time inventory tracker
‚îÇ       ‚îú‚îÄ‚îÄ analytics-service/    # Analytics aggregations
‚îÇ       ‚îî‚îÄ‚îÄ search-indexer/       # Search index updater
‚îÇ
‚îú‚îÄ‚îÄ scripts/                       # Automation scripts
‚îÇ   ‚îú‚îÄ‚îÄ schema.sql                # Database schema definition
‚îÇ   ‚îú‚îÄ‚îÄ init-database.sh          # Initialize RDS with schema
‚îÇ   ‚îú‚îÄ‚îÄ install-kafka.sh          # Install Kafka cluster via Helm
‚îÇ   ‚îú‚îÄ‚îÄ apply-service-accounts.sh # Deploy K8s service accounts
‚îÇ   ‚îî‚îÄ‚îÄ setup-argocd.sh           # Install and configure ArgoCD
‚îÇ
‚îú‚îÄ‚îÄ DEPLOYMENT.md                  # Detailed deployment guide
‚îî‚îÄ‚îÄ README.md                      # This file
```

## üõ†Ô∏è Prerequisites

### Required Tools
- **AWS CLI** (v2.x): AWS authentication and resource management
- **Terraform** (>= 1.5): Infrastructure provisioning
- **kubectl** (>= 1.28): Kubernetes CLI
- **Helm** (>= 3.12): Kubernetes package manager
- **ArgoCD CLI** (optional): GitOps management
- **Docker**: Container image builds
- **Git**: Version control

### AWS Requirements
- AWS account with appropriate IAM permissions
- Configured AWS credentials (`aws configure`)
- Available VPC and subnet capacity
- Service quota for EKS clusters

### Knowledge Prerequisites
- Kubernetes fundamentals
- Basic understanding of CDC concepts
- Terraform syntax
- GitOps principles

## üö¶ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/YOUR_ORG/RealTimeCDC-Platform.git
cd RealTimeCDC-Platform
```

### 2. Deploy Infrastructure
```bash
cd terraform
terraform init
terraform plan
terraform apply
```

**What gets created:**
- ‚úÖ VPC with public/private subnets across 3 AZs
- ‚úÖ EKS cluster (v1.33) with Karpenter autoscaling
- ‚úÖ RDS PostgreSQL (v16.3) with logical replication enabled
- ‚úÖ 3 S3 buckets (data lake, DLQ, Kafka Connect storage)
- ‚úÖ 6 IAM roles with IRSA for pod-level permissions
- ‚úÖ Security groups and networking
- ‚úÖ CloudWatch log groups

### 3. Configure kubectl
```bash
# Get kubeconfig command from Terraform output
terraform output configure_kubectl

# Run it (example output):
aws eks --region us-east-1 update-kubeconfig --name cdc-platform

# Verify access
kubectl get nodes
```

### 4. Deploy Karpenter NodePool
```bash
cd ..  # Back to project root

# Apply Karpenter NodePool configuration to enable worker node provisioning
kubectl apply -f k8s/karpenter/karpenter.yaml

# Wait for Karpenter to provision worker nodes (1-2 minutes)
kubectl get nodes -w
```

**Important**: Terraform deploys Karpenter itself, but doesn't apply the NodePool configuration. Without this step, only controller nodes exist (with taints), preventing regular workloads from scheduling.

### 5. Deploy Service Accounts
```bash
chmod +x scripts/apply-service-accounts.sh
./scripts/apply-service-accounts.sh
```

### 6. Initialize Database Schema
```bash
chmod +x scripts/init-database.sh
./scripts/init-database.sh
```

**What gets created:**
- ‚úÖ E-commerce tables: `products`, `orders`, `order_items`
- ‚úÖ Sample data: 5 products, 3 orders, 4 order items
- ‚úÖ CDC enabled: All tables configured with `REPLICA IDENTITY FULL`
- ‚úÖ Indexes for query performance
- ‚úÖ Foreign key relationships

**Alternative: Manual Schema Creation**
```bash
# Get RDS endpoint
cd terraform
terraform output rds_endpoint

# Connect using psql
kubectl run psql-client --rm -it --restart=Never \
  --image=postgres:16 --namespace=default \
  -- psql -h <rds-endpoint> -U dbadmin -d ecommerce

# Run the schema file
\i scripts/schema.sql
```

### 7. Install ArgoCD
```bash
chmod +x scripts/setup-argocd.sh
./scripts/setup-argocd.sh
```

### 8. Update Repository URLs
Update all files in `argocd/` directory:
```bash
# Find files to update
grep -r "YOUR_ORG" argocd/

# Replace YOUR_ORG with your GitHub username/org
# Files to update:
# - argocd/bootstrap/root-app.yaml
# - argocd/projects/cdc-platform.yaml
# - argocd/apps/*.yaml
```

### 9. Deploy Platform via ArgoCD
```bash
# Apply ArgoCD project
kubectl apply -f argocd/projects/cdc-platform.yaml

# Deploy root application (App of Apps)
kubectl apply -f argocd/bootstrap/root-app.yaml

# Access ArgoCD UI
kubectl port-forward svc/argocd-server -n argocd 8080:80

# GitHub Codespaces: Go to PORTS tab and click on port 8080 to open in browser
# Local: Open http://localhost:8080

# Login: admin / <password from setup script>
```

## üìä IAM Roles & Service Accounts (IRSA)

The platform uses IAM Roles for Service Accounts (IRSA) for secure, pod-level AWS permissions without static credentials.

### Available Roles

| Service Account | Namespace | AWS Permissions | Use Case |
|----------------|-----------|-----------------|----------|
| `kafka-connect` | kafka | S3 read/write, Secrets Manager read | Kafka Connect S3 sink connector |
| `debezium` | kafka | Secrets Manager read | Read RDS credentials |
| `inventory-service` | cdc-consumers | S3 read, DynamoDB write, Secrets read | Inventory consumer |
| `analytics-service` | cdc-consumers | S3 read, DynamoDB write, Secrets read | Analytics consumer |
| `search-indexer` | cdc-consumers | S3 read, DynamoDB write, Secrets read | Search indexer |
| `flink-jobmanager` | flink | S3 read/write, CloudWatch metrics | Flink job manager |
| `flink-taskmanager` | flink | S3 read/write, CloudWatch metrics | Flink task manager |
| `schema-registry` | kafka | S3 read/write (schemas) | Avro schema storage |
| `external-secrets` | external-secrets | Secrets Manager read | Sync AWS secrets to K8s |

### How IRSA Works
1. Terraform creates IAM role with trust policy for specific K8s service account
2. Service account is annotated with IAM role ARN
3. EKS Pod Identity webhook injects temporary credentials
4. Pods automatically get AWS access without static keys

## üóÑÔ∏è Database Schema

The platform uses a sample e-commerce schema with three main tables designed for Change Data Capture.

### Tables

#### Products Table
```sql
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    price DECIMAL(10,2) NOT NULL,
    stock_quantity INTEGER NOT NULL DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Orders Table
```sql
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    customer_id INTEGER NOT NULL,
    total_amount DECIMAL(10,2) NOT NULL,
    status VARCHAR(50) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Order Items Table
```sql
CREATE TABLE order_items (
    id SERIAL PRIMARY KEY,
    order_id INTEGER REFERENCES orders(id),
    product_id INTEGER REFERENCES products(id),
    quantity INTEGER NOT NULL,
    price DECIMAL(10,2) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### CDC Configuration

All tables are configured with:
- **REPLICA IDENTITY FULL**: Required for Debezium to capture full row state
- **Primary keys**: For row identification
- **Foreign key relationships**: Between orders, products, and order_items
- **Timestamps**: For audit tracking
- **Indexes**: For query performance

### RDS PostgreSQL Settings
- **WAL level**: `logical` (enables change data capture)
- **Max replication slots**: 10
- **Max WAL senders**: 25
- **RDS logical replication**: Enabled via parameter group

### Schema Files
- `scripts/schema.sql`: Complete schema definition with sample data
- `scripts/init-database.sh`: Automated initialization script

## üîÑ GitOps Workflow

### Deployment Sync Waves
ArgoCD deploys components in order using sync waves:

```
Wave 0: External Secrets Operator
   ‚Üì
Wave 1: Kafka Cluster, Prometheus/Grafana
   ‚Üì
Wave 2: Schema Registry, Flink Operator
   ‚Üì
Wave 3: Debezium Connectors
   ‚Üì
Wave 4: Consumer Services, Flink Jobs
```

### Making Changes

**Infrastructure Changes (Terraform):**
```bash
# 1. Update Terraform files
vim terraform/rds.tf

# 2. Apply changes
cd terraform
terraform plan
terraform apply

# 3. Update K8s service accounts if IAM roles changed
cd ..
./scripts/apply-service-accounts.sh
```

**Application Changes (Consumer Services):**
```bash
# 1. Update application code
vim apps/python/inventory-service/app.py

# 2. Build and push Docker image
cd apps/python/inventory-service
docker build -t <registry>/inventory-service:v1.1.0 .
docker push <registry>/inventory-service:v1.1.0

# 3. Update Kubernetes manifest
cd ../../../k8s/consumers/inventory-service
vim deployment.yaml  # Update image tag

# 4. Commit and push
git add deployment.yaml
git commit -m "Deploy inventory-service v1.1.0"
git push

# 5. ArgoCD auto-syncs (or manual sync)
argocd app sync inventory-service
```

**Kafka/Debezium Configuration:**
```bash
# 1. Update connector config
vim k8s/debezium/postgres-connector.yaml

# 2. Commit and push
git add k8s/debezium/
git commit -m "Increase Debezium batch size"
git push

# 3. ArgoCD detects and applies change
# Watch in UI or: argocd app sync debezium-connectors
```

## üìà Monitoring & Observability

### Accessing Grafana
```bash
kubectl port-forward svc/prometheus-operator-grafana -n monitoring 3000:80
# Open http://localhost:3000
# Login: admin / admin (change in production!)
```

### Pre-configured Dashboards
- Kafka cluster overview
- Kafka Connect connector status
- Consumer lag monitoring
- Flink job metrics
- Kubernetes cluster health

### Accessing Prometheus
```bash
kubectl port-forward svc/prometheus-operator-kube-prom-prometheus -n monitoring 9090:9090
# Open http://localhost:9090
```

### Key Metrics to Monitor
- **Kafka**: Topic lag, throughput, partition distribution
- **Debezium**: Connector status, snapshot progress, replication lag
- **Consumers**: Processing rate, error rate, DLQ events
- **Flink**: Checkpoint duration, backpressure, task status
- **RDS**: Replication slot lag, WAL generation rate

## üß™ Testing CDC Pipeline

### 1. Insert Test Data
```bash
# Get RDS endpoint
cd terraform
terraform output rds_endpoint

# Connect to database
kubectl run psql-client --rm -it --restart=Never \
  --image=postgres:16 --namespace=kafka \
  -- psql -h <rds-endpoint> -U dbadmin -d ecommerce

# Insert sample data
INSERT INTO products (name, description, price, stock_quantity) 
VALUES ('Test Product', 'CDC Test', 99.99, 100);

UPDATE products SET price = 89.99 WHERE name = 'Test Product';

DELETE FROM products WHERE name = 'Test Product';
```

### 2. Verify Kafka Topics
```bash
# List topics
kubectl exec -it cdc-platform-kafka-0 -n kafka -- bash
bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

# Expected topics:
# - dbserver1.public.products
# - dbserver1.public.orders
# - dbserver1.public.order_items

# Consume events
bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic dbserver1.public.products \
  --from-beginning
```

### 3. Check S3 Data Lake
```bash
aws s3 ls s3://cdc-platform-data-lake-<account-id>/topics/
```

### 4. Verify Consumer Processing
```bash
kubectl logs -l app=inventory-service -n cdc-consumers
kubectl logs -l app=analytics-service -n cdc-consumers
```

## üõ°Ô∏è Security Best Practices

### Implemented
- ‚úÖ VPC with private subnets for data plane
- ‚úÖ EKS cluster endpoint restricted (can enable private)
- ‚úÖ IRSA (no static AWS credentials)
- ‚úÖ Secrets stored in AWS Secrets Manager
- ‚úÖ RDS encryption at rest
- ‚úÖ S3 bucket encryption (AES-256)
- ‚úÖ Security groups with least privilege
- ‚úÖ Network policies (to be added)

### Production Recommendations
- [ ] Enable EKS private endpoint
- [ ] Implement Pod Security Standards
- [ ] Add AWS WAF for API protection
- [ ] Enable VPC Flow Logs
- [ ] Implement secrets rotation
- [ ] Add network policies between namespaces
- [ ] Enable audit logging (CloudTrail, EKS audit logs)
- [ ] Implement certificate management (cert-manager)

## üí∞ Cost Optimization

### Current Configuration (Dev)
- **EKS**: ~$73/month (control plane)
- **EC2**: ~$60/month (2x m5.large nodes)
- **RDS**: ~$25/month (db.t3.micro)
- **S3**: ~$5/month (with lifecycle policies)
- **Data Transfer**: Variable

**Estimated Total**: ~$165-200/month

### Cost Savings
- Karpenter uses Spot instances where possible
- S3 lifecycle policies move data to cheaper tiers
- RDS Multi-AZ disabled for dev
- Single NAT gateway in dev

### Production Considerations
- Enable RDS Multi-AZ for high availability
- Use reserved instances for baseline capacity
- Implement proper data retention policies
- Monitor and optimize S3 storage classes

## üîß Troubleshooting

### Debezium Not Capturing Changes
```bash
# Check connector status
kubectl get kafkaconnector -n kafka
kubectl describe kafkaconnector postgres-connector -n kafka

# Check connector logs
kubectl logs -l strimzi.io/cluster=kafka-connect -n kafka --tail=100

# Verify RDS logical replication
# Connect to RDS and run:
SHOW wal_level;  -- Should be 'logical'
SELECT * FROM pg_replication_slots;
```

### ArgoCD Application Out of Sync
```bash
# Check application status
argocd app get <app-name>

# View sync errors
argocd app logs <app-name>

# Force sync
argocd app sync <app-name> --force

# Refresh from Git
argocd app refresh <app-name>
```

### Pods Can't Access S3/RDS
```bash
# Verify service account has correct annotation
kubectl describe sa <service-account> -n <namespace>

# Check if pod has AWS credentials injected
kubectl exec -it <pod-name> -n <namespace> -- env | grep AWS

# Test AWS access from pod
kubectl exec -it <pod-name> -n <namespace> -- aws s3 ls
```

### Kafka Consumer Lag
```bash
# Check consumer group lag
kubectl exec -it cdc-platform-kafka-0 -n kafka -- \
  bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group <consumer-group> \
  --describe

# Scale consumer deployment
kubectl scale deployment/<consumer-name> -n cdc-consumers --replicas=3
```

## üßπ Cleanup

### Delete Kubernetes Resources
```bash
# Delete ArgoCD root app (cascades to all apps)
kubectl delete application cdc-platform-root -n argocd

# Or delete ArgoCD entirely
kubectl delete namespace argocd
```

### Destroy AWS Infrastructure
```bash
cd terraform
terraform destroy
```

**Warning**: This deletes:
- EKS cluster and all workloads
- RDS database (including data)
- S3 buckets (will fail if not empty)
- All IAM roles and policies

## üìö Additional Resources

### Documentation
- [Deployment Guide](DEPLOYMENT.md) - Detailed step-by-step deployment
- [Debezium Docs](https://debezium.io/documentation/reference/stable/connectors/postgresql.html)
- [Strimzi Kafka Operator](https://strimzi.io/docs/operators/latest/overview.html)
- [ArgoCD Best Practices](https://argo-cd.readthedocs.io/en/stable/user-guide/best_practices/)
- [Karpenter Documentation](https://karpenter.sh/docs/)

### AWS Resources
- [EKS Best Practices](https://aws.github.io/aws-eks-best-practices/)
- [RDS for PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html)
- [IRSA Documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)

## üéì Learning Objectives

This project demonstrates proficiency in:
- ‚úÖ Modern cloud-native architecture patterns
- ‚úÖ Infrastructure as Code with Terraform
- ‚úÖ Kubernetes orchestration and operations
- ‚úÖ GitOps and continuous delivery
- ‚úÖ Real-time data streaming and CDC
- ‚úÖ Event-driven microservices architecture
- ‚úÖ AWS services integration
- ‚úÖ Security best practices (IRSA, encryption, least privilege)
- ‚úÖ Monitoring and observability
- ‚úÖ Production-ready deployment patterns
---

**Status**: üöß Active Development

**Last Updated**: October 2025